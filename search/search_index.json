{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to my notes, thoughts and documentations!","text":"<p>This is a permanent location to capture notes, thoughts, learnings and ideas.</p>"},{"location":"about/","title":"About","text":"<p>Still thinking about what to put down for this section ... ^_^</p>"},{"location":"ai/","title":"AI","text":""},{"location":"ai/ai_agents/","title":"AI Agents","text":""},{"location":"ai/ai_agents/common_functions/","title":"Common Functions for AI Agents","text":""},{"location":"ai/ai_agents/common_functions/#basic-setup-functions-and-code","title":"Basic Setup Functions and Code","text":"<p>Some common functions are the following (source code here). The basic ideas are summarized here:</p> <ul> <li><code>generate_response</code> is the main function to call. One can use it independently, but more structurally it's used in this agent framework following GAME (Goal, Action, Model &amp; Environment) components:<ul> <li>The <code>Agent</code> class<ul> <li>Receives <code>Goal</code>, <code>AgentLanguage</code>, <code>ActionRegistry</code> and <code>Environment</code> classes as configurational arguments.</li> <li><code>run</code> is the major method. It typically follows the procedures of<ol> <li>Setting up the task based on user input (via <code>set_current_task</code> method)</li> <li>Construct the proper prompt based on the goals, actions and current memory (via <code>construct_prompt</code> method)</li> <li>Receive action/decision from prompt (via <code>prompt_llm_for_action</code> method)</li> <li>Execute the action in the environment (via <code>Environment</code>'s <code>execute_action</code> method)</li> <li>Update agent's memory (via <code>update_memory</code> method)</li> <li>Decide if it should terminate (via <code>should_terminate</code> method)</li> <li>If to terminate, return memory; otherwise, go back to Step 2</li> </ol> </li> </ul> </li> <li>The <code>Goal</code> class<ul> <li>Receives <code>priority</code> (integer), <code>name</code> (string) and <code>description</code> (string) as arguments.</li> </ul> </li> <li>The <code>Action</code> class<ul> <li>Receives <code>name</code> (string), <code>function</code> (callable function), <code>description</code> (string), <code>terminal</code> (boolean, whether to terminate after the action) and <code>parameters</code> (dict) as arguments.</li> <li>The <code>execute</code> method executes the action function.</li> <li>Note that we don't explicitly \"write out\" the action class instances, but rather this is done via <code>PythonActionRegistry</code> and the <code>register_tool</code> decorator which is applied to each action function.</li> </ul> </li> <li>The <code>ActionRegistry</code> and <code>PythonRegistry</code> classes<ul> <li>Think the so-called registry as catalog of all actions, each optionally (but preferably) tagged with a \"type\".</li> <li>It needs a <code>tools</code> global parameter to store all the tools, via the <code>register_tools</code> decorator.</li> <li>The registry, upon specifying the relevant tags, would register each action in the registry so LLM knows to use them.</li> </ul> </li> <li>The <code>Memory</code> class<ul> <li>It handles the storage of all conversation information, including <code>add_memory</code>, <code>get_memories</code>, and <code>copy_without_system_memories</code></li> </ul> </li> <li>The <code>Environment</code> class<ul> <li>This is where actions are executed and results are properly formatted.</li> </ul> </li> <li>The <code>AgentFunctionCallingActionLanguage</code> class (and potentially other <code>AgentLanguage</code> classes in general)<ul> <li>This is to define how the agent responds to which action to take (hence agent language)</li> </ul> </li> </ul> </li> </ul> <pre><code>import json\nimport time\nimport traceback\nimport inspect\nfrom litellm import completion\nfrom dataclasses import dataclass, field\nfrom typing import get_type_hints, List, Callable, Dict, Any\n\ntools = {}\ntools_by_tag = {}\n\n\ndef to_openai_tools(tools_metadata: List[dict]):\n    openai_tools = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": t['tool_name'],\n                # Include up to 1024 characters of the description\n                \"description\": t.get('description',\"\")[:1024],\n                \"parameters\": t.get('parameters',{}),\n            },\n        } for t in tools_metadata\n    ]\n    return openai_tools\n\ndef get_tool_metadata(func, tool_name=None, description=None, parameters_override=None, terminal=False, tags=None):\n    \"\"\"\n    Extracts metadata for a function to use in tool registration.\n\n    Parameters:\n        func (function): The function to extract metadata from.\n        tool_name (str, optional): The name of the tool. Defaults to the function name.\n        description (str, optional): Description of the tool. Defaults to the function's docstring.\n        parameters_override (dict, optional): Override for the argument schema. Defaults to dynamically inferred schema.\n        terminal (bool, optional): Whether the tool is terminal. Defaults to False.\n        tags (List[str], optional): List of tags to associate with the tool.\n\n    Returns:\n        dict: A dictionary containing metadata about the tool, including description, args schema, and the function.\n    \"\"\"\n    # Default tool_name to the function name if not provided\n    tool_name = tool_name or func.__name__\n\n    # Default description to the function's docstring if not provided\n    description = description or (func.__doc__.strip() if func.__doc__ else \"No description provided.\")\n\n    # Discover the function's signature and type hints if no args_override is provided\n    if parameters_override is None:\n        signature = inspect.signature(func)\n        type_hints = get_type_hints(func)\n\n        # Build the arguments schema dynamically\n        args_schema = {\n            \"type\": \"object\",\n            \"properties\": {},\n            \"required\": []\n        }\n        for param_name, param in signature.parameters.items():\n\n            if param_name in [\"action_context\", \"action_agent\"]:\n                continue  # Skip these parameters\n\n            def get_json_type(param_type):\n                if param_type == str:\n                    return \"string\"\n                elif param_type == int:\n                    return \"integer\"\n                elif param_type == float:\n                    return \"number\"\n                elif param_type == bool:\n                    return \"boolean\"\n                elif param_type == list:\n                    return \"array\"\n                elif param_type == dict:\n                    return \"object\"\n                else:\n                    return \"string\"\n\n            # Add parameter details\n            param_type = type_hints.get(param_name, str)  # Default to string if type is not annotated\n            param_schema = {\"type\": get_json_type(param_type)}  # Convert Python types to JSON schema types\n\n            args_schema[\"properties\"][param_name] = param_schema\n\n            # Add to required if not defaulted\n            if param.default == inspect.Parameter.empty:\n                args_schema[\"required\"].append(param_name)\n    else:\n        args_schema = parameters_override\n\n    # Return the metadata as a dictionary\n    return {\n        \"tool_name\": tool_name,\n        \"description\": description,\n        \"parameters\": args_schema,\n        \"function\": func,\n        \"terminal\": terminal,\n        \"tags\": tags or []\n    }\n\n\ndef register_tool(tool_name=None, description=None, parameters_override=None, terminal=False, tags=None):\n    \"\"\"\n    A decorator to dynamically register a function in the tools dictionary with its parameters, schema, and docstring.\n\n    Parameters:\n        tool_name (str, optional): The name of the tool to register. Defaults to the function name.\n        description (str, optional): Override for the tool's description. Defaults to the function's docstring.\n        parameters_override (dict, optional): Override for the argument schema. Defaults to dynamically inferred schema.\n        terminal (bool, optional): Whether the tool is terminal. Defaults to False.\n        tags (List[str], optional): List of tags to associate with the tool.\n\n    Returns:\n        function: The wrapped function.\n    \"\"\"\n    def decorator(func):\n        # Use the reusable function to extract metadata\n        metadata = get_tool_metadata(\n            func=func,\n            tool_name=tool_name,\n            description=description,\n            parameters_override=parameters_override,\n            terminal=terminal,\n            tags=tags\n        )\n\n        # Register the tool in the global dictionary\n        tools[metadata[\"tool_name\"]] = {\n            \"description\": metadata[\"description\"],\n            \"parameters\": metadata[\"parameters\"],\n            \"function\": metadata[\"function\"],\n            \"terminal\": metadata[\"terminal\"],\n            \"tags\": metadata[\"tags\"] or []\n        }\n\n        for tag in metadata[\"tags\"]:\n            if tag not in tools_by_tag:\n                tools_by_tag[tag] = []\n            tools_by_tag[tag].append(metadata[\"tool_name\"])\n\n        return func\n    return decorator\n\n\n@dataclass\nclass Prompt:\n    messages: List[Dict] = field(default_factory=list)\n    tools: List[Dict] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)  # Fixing mutable default issue\n\n\ndef generate_response(prompt: Prompt) -&gt; str:\n    \"\"\"Call LLM to get response\"\"\"\n\n    messages = prompt.messages\n    tools = prompt.tools\n\n    result = None\n\n    if not tools:\n        response = completion(\n            model=\"openai/gpt-4o\",\n            messages=messages,\n            max_tokens=1024\n        )\n        result = response.choices[0].message.content\n    else:\n        response = completion(\n            model=\"openai/gpt-4o\",\n            messages=messages,\n            tools=tools,\n            max_tokens=1024\n        )\n\n        if response.choices[0].message.tool_calls:\n            tool = response.choices[0].message.tool_calls[0]\n            result = {\n                \"tool\": tool.function.name,\n                \"args\": json.loads(tool.function.arguments),\n            }\n            result = json.dumps(result)\n        else:\n            result = response.choices[0].message.content\n\n\n    return result\n\n\n@dataclass(frozen=True)\nclass Goal:\n    priority: int\n    name: str\n    description: str\n\n\nclass Action:\n    def __init__(self,\n                 name: str,\n                 function: Callable,\n                 description: str,\n                 parameters: Dict,\n                 terminal: bool = False):\n        self.name = name\n        self.function = function\n        self.description = description\n        self.terminal = terminal\n        self.parameters = parameters\n\n    def execute(self, **args) -&gt; Any:\n        \"\"\"Execute the action's function\"\"\"\n        return self.function(**args)\n\n\nclass ActionRegistry:\n    def __init__(self):\n        self.actions = {}\n\n    def register(self, action: Action):\n        self.actions[action.name] = action\n\n    def get_action(self, name: str) -&gt; [Action, None]:\n        return self.actions.get(name, None)\n\n    def get_actions(self) -&gt; List[Action]:\n        \"\"\"Get all registered actions\"\"\"\n        return list(self.actions.values())\n\n\nclass Memory:\n    def __init__(self):\n        self.items = []  # Basic conversation histor\n\n    def add_memory(self, memory: dict):\n        \"\"\"Add memory to working memory\"\"\"\n        self.items.append(memory)\n\n    def get_memories(self, limit: int = None) -&gt; List[Dict]:\n        \"\"\"Get formatted conversation history for prompt\"\"\"\n        return self.items[:limit]\n\n    def copy_without_system_memories(self):\n        \"\"\"Return a copy of the memory without system memories\"\"\"\n        filtered_items = [m for m in self.items if m[\"type\"] != \"system\"]\n        memory = Memory()\n        memory.items = filtered_items\n        return memory\n\n\nclass Environment:\n    def execute_action(self, action: Action, args: dict) -&gt; dict:\n        \"\"\"Execute an action and return the result.\"\"\"\n        try:\n            result = action.execute(**args)\n            return self.format_result(result)\n        except Exception as e:\n            return {\n                \"tool_executed\": False,\n                \"error\": str(e),\n                \"traceback\": traceback.format_exc()\n            }\n\n    def format_result(self, result: Any) -&gt; dict:\n        \"\"\"Format the result with metadata.\"\"\"\n        return {\n            \"tool_executed\": True,\n            \"result\": result,\n            \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n        }\n\n\nclass AgentLanguage:\n    def __init__(self):\n        pass\n\n    def construct_prompt(self,\n                         actions: List[Action],\n                         environment: Environment,\n                         goals: List[Goal],\n                         memory: Memory) -&gt; Prompt:\n        raise NotImplementedError(\"Subclasses must implement this method\")\n\n\n    def parse_response(self, response: str) -&gt; dict:\n        raise NotImplementedError(\"Subclasses must implement this method\")\n\n\n\nclass AgentFunctionCallingActionLanguage(AgentLanguage):\n\n    def __init__(self):\n        super().__init__()\n\n    def format_goals(self, goals: List[Goal]) -&gt; List:\n        # Map all goals to a single string that concatenates their description\n        # and combine into a single message of type system\n        sep = \"\\n-------------------\\n\"\n        goal_instructions = \"\\n\\n\".join([f\"{goal.name}:{sep}{goal.description}{sep}\" for goal in goals])\n        return [\n            {\"role\": \"system\", \"content\": goal_instructions}\n        ]\n\n    def format_memory(self, memory: Memory) -&gt; List:\n        \"\"\"Generate response from language model\"\"\"\n        # Map all environment results to a role:user messages\n        # Map all assistant messages to a role:assistant messages\n        # Map all user messages to a role:user messages\n        items = memory.get_memories()\n        mapped_items = []\n        for item in items:\n\n            content = item.get(\"content\", None)\n            if not content:\n                content = json.dumps(item, indent=4)\n\n            if item[\"type\"] == \"assistant\":\n                mapped_items.append({\"role\": \"assistant\", \"content\": content})\n            elif item[\"type\"] == \"environment\":\n                mapped_items.append({\"role\": \"assistant\", \"content\": content})\n            else:\n                mapped_items.append({\"role\": \"user\", \"content\": content})\n\n        return mapped_items\n\n    def format_actions(self, actions: List[Action]) -&gt; [List,List]:\n        \"\"\"Generate response from language model\"\"\"\n\n        tools = [\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": action.name,\n                    # Include up to 1024 characters of the description\n                    \"description\": action.description[:1024],\n                    \"parameters\": action.parameters,\n                },\n            } for action in actions\n        ]\n\n        return tools\n\n    def construct_prompt(self,\n                         actions: List[Action],\n                         environment: Environment,\n                         goals: List[Goal],\n                         memory: Memory) -&gt; Prompt:\n\n        prompt = []\n        prompt += self.format_goals(goals)\n        prompt += self.format_memory(memory)\n\n        tools = self.format_actions(actions)\n\n        return Prompt(messages=prompt, tools=tools)\n\n    def adapt_prompt_after_parsing_error(self,\n                                         prompt: Prompt,\n                                         response: str,\n                                         traceback: str,\n                                         error: Any,\n                                         retries_left: int) -&gt; Prompt:\n\n        return prompt\n\n    def parse_response(self, response: str) -&gt; dict:\n        \"\"\"Parse LLM response into structured format by extracting the ```json block\"\"\"\n\n        try:\n            return json.loads(response)\n\n        except Exception as e:\n            return {\n                \"tool\": \"terminate\",\n                \"args\": {\"message\":response}\n            }\n\n\n\nclass PythonActionRegistry(ActionRegistry):\n    def __init__(self, tags: List[str] = None, tool_names: List[str] = None):\n        super().__init__()\n\n        self.terminate_tool = None\n\n        for tool_name, tool_desc in tools.items():\n            if tool_name == \"terminate\":\n                self.terminate_tool = tool_desc\n\n            if tool_names and tool_name not in tool_names:\n                continue\n\n            tool_tags = tool_desc.get(\"tags\", [])\n            if tags and not any(tag in tool_tags for tag in tags):\n                continue\n\n            self.register(Action(\n                name=tool_name,\n                function=tool_desc[\"function\"],\n                description=tool_desc[\"description\"],\n                parameters=tool_desc.get(\"parameters\", {}),\n                terminal=tool_desc.get(\"terminal\", False)\n            ))\n\n    def register_terminate_tool(self):\n        if self.terminate_tool:\n            self.register(Action(\n                name=\"terminate\",\n                function=self.terminate_tool[\"function\"],\n                description=self.terminate_tool[\"description\"],\n                parameters=self.terminate_tool.get(\"parameters\", {}),\n                terminal=self.terminate_tool.get(\"terminal\", False)\n            ))\n        else:\n            raise Exception(\"Terminate tool not found in tool registry\")\n\n\n\nclass Agent:\n    def __init__(self,\n                 goals: List[Goal],\n                 agent_language: AgentLanguage,\n                 action_registry: ActionRegistry,\n                 generate_response: Callable[[Prompt], str],\n                 environment: Environment):\n        \"\"\"\n        Initialize an agent with its core GAME components\n        \"\"\"\n        self.goals = goals\n        self.generate_response = generate_response\n        self.agent_language = agent_language\n        self.actions = action_registry\n        self.environment = environment\n\n    def construct_prompt(self, goals: List[Goal], memory: Memory, actions: ActionRegistry) -&gt; Prompt:\n        \"\"\"Build prompt with memory context\"\"\"\n        return self.agent_language.construct_prompt(\n            actions=actions.get_actions(),\n            environment=self.environment,\n            goals=goals,\n            memory=memory\n        )\n\n    def get_action(self, response):\n        invocation = self.agent_language.parse_response(response)\n        action = self.actions.get_action(invocation[\"tool\"])\n        return action, invocation\n\n    def should_terminate(self, response: str) -&gt; bool:\n        action_def, _ = self.get_action(response)\n        return action_def.terminal\n\n    def set_current_task(self, memory: Memory, task: str):\n        memory.add_memory({\"type\": \"user\", \"content\": task})\n\n    def update_memory(self, memory: Memory, response: str, result: dict):\n        \"\"\"\n        Update memory with the agent's decision and the environment's response.\n        \"\"\"\n        new_memories = [\n            {\"type\": \"assistant\", \"content\": response},\n            {\"type\": \"environment\", \"content\": json.dumps(result)}\n        ]\n        for m in new_memories:\n            memory.add_memory(m)\n\n    def prompt_llm_for_action(self, full_prompt: Prompt) -&gt; str:\n        response = self.generate_response(full_prompt)\n        return response\n\n    def run(self, user_input: str, memory=None, max_iterations: int = 50) -&gt; Memory:\n        \"\"\"\n        Execute the GAME loop for this agent with a maximum iteration limit.\n        \"\"\"\n        memory = memory or Memory()\n        self.set_current_task(memory, user_input)\n\n        for _ in range(max_iterations):\n            # Construct a prompt that includes the Goals, Actions, and the current Memory\n            prompt = self.construct_prompt(self.goals, memory, self.actions)\n\n            print(\"Agent thinking...\")\n            # Generate a response from the agent\n            response = self.prompt_llm_for_action(prompt)\n            print(f\"Agent Decision: {response}\")\n\n            # Determine which action the agent wants to execute\n            action, invocation = self.get_action(response)\n\n            # Execute the action in the environment\n            result = self.environment.execute_action(action, invocation[\"args\"])\n            print(f\"Action Result: {result}\")\n\n            # Update the agent's memory with information about what happened\n            self.update_memory(memory, response, result)\n\n            # Check if the agent has decided to terminate\n            if self.should_terminate(response):\n                break\n\n        return memory\n</code></pre>"},{"location":"ai/ai_agents/common_functions/#example","title":"Example","text":"<p>Here's an actual example on how to use this framework:</p> <pre><code># First, we'll define our tools using decorators\n@register_tool(tags=[\"file_operations\", \"read\"])\ndef read_project_file(name: str) -&gt; str:\n    \"\"\"Reads and returns the content of a specified project file.\n\n    Opens the file in read mode and returns its entire contents as a string.\n    Raises FileNotFoundError if the file doesn't exist.\n\n    Args:\n        name: The name of the file to read\n\n    Returns:\n        The contents of the file as a string\n    \"\"\"\n    with open(name, \"r\") as f:\n        return f.read()\n\n@register_tool(tags=[\"file_operations\", \"list\"])\ndef list_project_files() -&gt; List[str]:\n    \"\"\"Lists all Python files in the current project directory.\n\n    Scans the current directory and returns a sorted list of all files\n    that end with '.py'.\n\n    Returns:\n        A sorted list of Python filenames\n    \"\"\"\n    return sorted([file for file in os.listdir(\".\")\n                    if file.endswith(\".py\")])\n\n@register_tool(tags=[\"system\"], terminal=True)\ndef terminate(message: str) -&gt; str:\n    \"\"\"Terminates the agent's execution with a final message.\n\n    Args:\n        message: The final message to return before terminating\n\n    Returns:\n        The message with a termination note appended\n    \"\"\"\n    return f\"{message}\\nTerminating...\"\n\n\n# Define the agent's goals\ngoals = [\n    Goal(priority=1,\n            name=\"Gather Information\",\n            description=\"Read each file in the project in order to build a deep understanding of the project in order to write a README\"),\n    Goal(priority=1,\n            name=\"Terminate\",\n            description=\"Call terminate when done and provide a complete README for the project in the message parameter\")\n]\n\n# Create an agent instance with tag-filtered actions\nagent = Agent(\n    goals=goals,\n    agent_language=AgentFunctionCallingActionLanguage(),\n    # The ActionRegistry now automatically loads tools with these tags\n    action_registry=PythonActionRegistry(tags=[\"file_operations\", \"system\"]),\n    generate_response=generate_response,\n    environment=Environment()\n)\n\n# Run the agent with user input\nuser_input = \"Write a README for this project.\"\nfinal_memory = agent.run(user_input)\nprint(final_memory.get_memories())\n</code></pre> <p>The example has three tools set up: - <code>read_project_file</code>: it reads a file given the directory - <code>list_project_files</code>: it lists all Python files from a given directory, sorted - <code>terminate</code>: terminate the process</p> <p>The goal of the agent is set to read all files and write a README.</p>"},{"location":"cloud/","title":"Cloud Related","text":""},{"location":"data_engineering/","title":"Data Engineering","text":"<p>Some notes, tips and ideas regarding Data Engineering are documented here.</p>"},{"location":"data_engineering/databases/","title":"Databases","text":""},{"location":"data_engineering/databases/database_access/","title":"Database Access","text":""},{"location":"data_engineering/databases/database_access/#access-snowflake-database","title":"Access Snowflake Database","text":"<p>A simple way to access Snowflake database locally using Python is as follows:</p> <pre><code>import snowflake.connector\n\ncon = snowflake.connector.connect(\n    user=\"&lt;user-name&gt;\",\n    account=\"&lt;snowflake-account&gt;\",  # at 3M it's mmm-ww.privatelink\n    authenticator=\"externalbrowser\",\n    session_parameters={\"QUERY_TAG\": \"my test\"}\n)\ncs = con.cursor()\n\n# query script\ncontent = cs.execute(\n    \"\"\"\n    SELECT * FROM table\n    \"\"\"\n).fetchall()\n\n### Some additional actions ###\n\ncs.close()\ncon.close()\n</code></pre> <p>To get data as a Pandas dataframe, do the following:</p> <pre><code>import pandas as pd\n\ndf = pd.DataFrame(content, columns=[col[0] for col in cs.description])\n</code></pre>"},{"location":"data_engineering/databases/database_access/#access-sql-server-database-on-mac","title":"Access SQL Server database (on Mac)","text":"<p>It's a much simpler process to access SQL Server database on Windows, but on Mac it can be tricky. Below is a method that's proven to work.</p> <p>First we need a bunch of information for the database:</p> <pre><code>import getpass\n\nSERVER = \"&lt;server&gt;\"\nPORT = \"1433\"  # 1433 is often a good starting one\nDATABASE = \"&lt;database-name&gt;\"\nDOMAIN = \"&lt;domain-name&gt;\"\nUSER = \"&lt;username&gt;\"\nPASSWORD = getpass.getpass(\"Enter password: \")  # getpass would prompt for user to enter directly\n</code></pre> <p>Now, we need to go to JTDS to download a driver: https://jtds.sourceforge.net and keep its directory:</p> <pre><code>JTDS_JAR = \"&lt;directory-of-jar-file&gt;\"\n</code></pre> <p>Now we can query with the code below:</p> <pre><code>import jaydebeapi\nimport csv\n\njdbc_url = (\n    f\"jdbc:jtds:sqlserver://{SERVER}:{PORT}/{DATABASE};\"\n    f\"useNTLMv2=true;domain={DOMAIN};\"\n)\nquery_res_file = \"result.csv\"\n\ncon = jaydebeapi.connect(\n    \"net.sourceforge.jtds.jdbc.Driver\",\n    jdbc_url, [USER, PASSWORD], JTDS_JAR\n)\ncs = con.cursor()\ncs.execute(\n    \"\"\"\n    SELECT * FROM table\n    \"\"\"\n)\n\n# fetch all rows\nrows = cs.fetchall()\n# get column headers\nheaders = [desc[0] for desc in cs.description]\n# write to csv\nwith open(query_res_file, \"w\", newline=\"\") as f:\n    writer = csv.writer(f)\n    writer.writerow(headers)\n    writer.writerows(rows)\n</code></pre>"},{"location":"data_engineering/databricks/","title":"Databricks","text":""},{"location":"data_science/","title":"Data Science","text":""},{"location":"python/","title":"Python","text":""},{"location":"tips_tricks/","title":"Tips &amp; Tricks","text":""}]}